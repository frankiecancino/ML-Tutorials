{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is on the basics of logistic regression. It is also a continuation of the Intro to Machine Learning post, \"What is Linear Regression?\", which can be found [here.](https://towardsdatascience.com/what-is-linear-regression-e44d2c4bf025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a little counterintuitive, but Logistic Regression is typically used as a classifier. In fact, Logistic Regression is one of the most used and well-known classification methods Data Scientists use. The idea behind this classification method is that the output will be between 0 and 1. Essentially returning the probability that the data you gave to the model, belongs to a certain group or class. From there the developer can set thresholds, depending on how much tolerance for error they are willing to give. \n",
    "\n",
    "For example, I may set a threshold of 0.8. Which means any output from the Logistic Regression model equal to or greater than 0.8 will be classified as a 1, anything less will be classified as a 0. From there I can move that 0.8 threshold up or down depending on my use case and the metrics I care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Logistic Regression relate to Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Regression formula is included in the Logistic Regression formula. \n",
    "If you recall, the Linear Regression formula is: $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 ... + \\beta_n x_n + \\epsilon $$\n",
    "\n",
    "Well...the formula for Logistic Regession is: $$ p = \\frac{1}{1 + e^{-y}} $$\n",
    "\n",
    "And we can swap out the `y` with our Linear Regression formula: $$ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 ... + \\beta_n x_n)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this equation doing?\n",
    "What is happening is that the Linear Regression provides some output value and the Logistic Regression portion pushes these values between 0 and 1 (inclusive). This forms somewhat of an S-curve which you may have seen before.\n",
    "\n",
    "If not, here is an example from the Scikit Learn docs:\n",
    "\n",
    "![s-vurve](https://scikit-learn.org/stable/_images/sphx_glr_plot_logistic_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Go Through An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we really just need Scikit Learn. If you aren't familiar with Scikit Learn, it is one of the most popular Python libraries to run machine learning algorithms. I am going to create some fake sample data about patients and if they should be approved for some type of treatment. I will be creating the columns for age, weight, average resting heart rate, and then for the approval decision. In this example we will use all of the columns, besides the approval decision as our features. Our model should estimate how important each feature is towards the approval decision and we should get a probability of the patient being approved for the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I will generate some sample data and use Numpy to store the data in an object. This isn't specifically needed to run Logistic Regression, but are used just for the sake of the example. Also, this data is completely made up. This is not indicitave of any real treatments or decision making by medical professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are importing the libraries Numpy and Scikit Learn. Numpy is used to create data structures, while scikit learn is used for a lot of machine learning use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy to create numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import Scikit-Learn to allow us to run Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating some sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have created here is some fake sample data and their respective features associated with age, weight, and average heart rate.\n",
    "Our approval decision will be a column called `approved` and will contain 0 or 1's indicating whether or not the patient was approved. 0 means the patient was not approved, 1 meaning the patient was approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample data\n",
    "approved = np.array([1, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "age = [21, 42, 35, 33, 63, 70, 26, 31, 52, 53]\n",
    "weight = [110, 180, 175, 235, 95, 90, 175, 190, 250, 185]\n",
    "avg_hrt = [65, 70, 72, 77, 67, 62, 68, 65, 73, 75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines are to shape the data properly to use with scikit learn. We stack the multiple arrays (or the x's) into one numpy object. While also specifying the \"shape\" of the price array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the multiple lists into one object called \"X\"\n",
    "X = np.column_stack([age, weight, avg_hrt])\n",
    "# Reshaping the approvals to work with scikit learn\n",
    "y = approved.reshape(len(approved), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the shell of the logistic regression model using this line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model object\n",
    "model = LogisticRegression(fit_intercept = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the shell, we need to fit the model with data. This is how we can get the Beta values and find some function that can approximate a patient's approval status - given their age, weight, and average resting heart rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with data\n",
    "fitted_model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a Logistic Regression model! Now let's take a look at the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient, or the beta one value, for age = -0.2820214641206744\n",
      "The coefficient, or the beta two value, for weight = -0.06858172570968078\n",
      "The coefficient, or the beta three value, for average resting heart rate = 0.3403873155933503\n"
     ]
    }
   ],
   "source": [
    "# Printing out the coefficients for each feature and the intercept\n",
    "print(f\"The coefficient, or the beta one value, for age = {fitted_model.coef_[0][0]}\")\n",
    "print(f\"The coefficient, or the beta two value, for weight = {fitted_model.coef_[0][1]}\")\n",
    "print(f\"The coefficient, or the beta three value, for average resting heart rate = {fitted_model.coef_[0][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model & its coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What these coefficients are telling us, in the case of classification, is that age is the most significant factor. Closely followed by average resting heart rate. With the patient's weight coming in last, for this specific data + case. This is useful to understand what features or components contribute to your model and impact your decisions. When talking about explainable AI, typically having a way of intepreting the model and seeing how the model came to its decision is extremely important. In this case we can look at the coefficients to determine how impactful each feature was and why an algorithm may choose to approve someone, but not others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the model on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_age = [20, 45, 33, 31, 62, 71, 72, 25, 30, 53, 55]\n",
    "new_weight = [105, 175, 170, 240, 100, 95, 200, 170, 195, 255, 180]\n",
    "new_avg_hrt = [64, 68, 70, 78, 67, 61, 68, 67, 66, 75, 76]\n",
    "\n",
    "# Combining the multiple lists into one object called \"test_X\"\n",
    "test_X = np.column_stack([new_age, new_weight, new_avg_hrt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run new data through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fitted_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our approval results are: [1 1 1 0 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our approval results are: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Scikit Learn automatically set a threshold for us and determined our approvals. If you want to look at the actual probabilities, we can use a different function provided by Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our approval results with their probabilites:\n",
      "Probability of being 0 (not approved) = 0.00, Probability of being 1 (approved) = 1.00\n",
      "Probability of being 0 (not approved) = 0.28, Probability of being 1 (approved) = 0.72\n",
      "Probability of being 0 (not approved) = 0.00, Probability of being 1 (approved) = 1.00\n",
      "Probability of being 0 (not approved) = 0.84, Probability of being 1 (approved) = 0.16\n",
      "Probability of being 0 (not approved) = 0.92, Probability of being 1 (approved) = 0.08\n",
      "Probability of being 0 (not approved) = 0.99, Probability of being 1 (approved) = 0.01\n",
      "Probability of being 0 (not approved) = 1.00, Probability of being 1 (approved) = 0.00\n",
      "Probability of being 0 (not approved) = 0.00, Probability of being 1 (approved) = 1.00\n",
      "Probability of being 0 (not approved) = 0.00, Probability of being 1 (approved) = 1.00\n",
      "Probability of being 0 (not approved) = 1.00, Probability of being 1 (approved) = 0.00\n",
      "Probability of being 0 (not approved) = 1.00, Probability of being 1 (approved) = 0.00\n"
     ]
    }
   ],
   "source": [
    "results_w_probs = fitted_model.predict_proba(test_X)\n",
    "print(\"Our approval results with their probabilites:\")\n",
    "for result in results_w_probs:\n",
    "    print(f\"Probability of being 0 (not approved) = {result[0]:.2f}, Probability of being 1 (approved) = {result[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The list of probablilites for the 1's and 0's are in the same order as our initial numpy array we passed to the `predict_proba()` function earlier.\n",
    "\n",
    "From here we can set different thresholds to provide different number of approvals based on probability thresholds. If you want to be cautious with approving people, we can set the threshold to 0.8 or 0.9. If the treatment is safe, non-invasive, and has a low cost - we can set the threshold lower to 0.25 or 0.3. This all depends on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would you do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some next steps on how to make your model better include:\n",
    "- Adding more data.\n",
    "- Normalize your data. Scale each column between 0 and 1 or between -1 and 1 to help the model. It makes learning difficult for the model when you don't scale your data since it could weigh features, that just have higher values naturally, more than others.\n",
    "- Test out different thresholds.\n",
    "- Learn about classification metrics. (Scikit Learn's [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) is a good place to start)\n",
    "- Tailor your model towards classification metrics that make sense for your use case. Accuracy may not be the best metric. (An example would be when you have imbalanced classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this tutorial was informative and you can take away learnings about Logistic Regression. Logsitic Regression is a fundamental tool in any Data Scientist's tool belt. You can look at coefficients and attempt to learn what data is impacting your model. You can use Logistic Regression as a baseline tool to determine if other models may be better. Logistic Regression is quick to train and doesn't require a lot of data. Although, in some cases it can be too simple. Therefore, don't count out more complex methods or additional methods to make your model better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankie Cancino is a Senior AI Scientist for Target and the founder of the Data Science Minneapolis group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "* [Scikit-Learn Docs](https://scikit-learn.org/stable/)\n",
    "* [LinkedIn](https://www.linkedin.com/in/frankie-cancino/)\n",
    "* [Twitter](https://twitter.com/frankiecancino)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
