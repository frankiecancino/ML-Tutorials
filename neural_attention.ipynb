{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have heard of Generative AI, ChatGPT, and/or Transformers. These advancements in AI came from decades of research building upon each other. One piece of research that was fundamental to these advancements was the Neural Attention function, introduced by [Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to the work on Neural Attention, the state of the art methods for Natural Language Processing (NLP) were to use Recurrent Neural Networks (RNN's) to encode text to a fixed-length vector and then decode the fixed-length vector to different text. A typical example of this is translation between languages. The original text in one language (let's say English) can be encoded into a fixed-length vector, then decoded from this fixed-length vector into a different language (such as French). This is called an encoder-decoder approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Neural Attention work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Attention doesn't require a function or model to encode the entire input text. Instead, the idea is to focus on words or inputs more relevant for predicting the next word. For example - when translating each new word, a model using neural attention will use the important information from the input and the already-translated words. This allows models to handle longer inputs more efficiently, since the \"attention\" is selective and not requiring the model to rely on the entire input.\n",
    "\n",
    "To calculate a basic form of attention, we will require 3 inputs. Queries (Q), Keys (K), and Values (V). These 3 inputs will vary depending on use case and what data a model is being trained on, but you can think of the inputs in this way:\n",
    "- (Q)ueries: is the input text\n",
    "- (K)eys: positions that may or may not contain relevant information\n",
    "- (V)alues: the potentially relevant information to be used to generate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product Attention in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example in Python, we use Tensorflow and its `einsum` and `softmax` functions. `einsum` essentially conducts matrix/vector multiplication for arbitrary multi-dimensional sizes of matrices or tensors. `softmax` will generate the weights for the keys or positions, adding up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def dot_product_attention(q, K, V): \n",
    "    \"\"\" Dot−Product Attention on a single query. \n",
    "    Args:\n",
    "        q: a vector with shape [k]\n",
    "        K: a matrix with shape [m, k] \n",
    "        V: a matrix with shape [m, v]\n",
    "    Returns:\n",
    "        y: a vector with shape [v]\n",
    "    \"\"\"\n",
    "    # calculate the logits from the query and associated keys to pass to softmax\n",
    "    logits = tf.einsum(\"k,mk−>m\", q, K)\n",
    "    # generate the weights based off of which keys were impactful\n",
    "    weights = tf.softmax(logits)\n",
    "    # results are the weights multiplied by the values to generate a vector\n",
    "    # this vector tells us what values were important\n",
    "    y = tf.einsum(\"m,mv−>v\", weights, V)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple implementation of attention looks at a query and keys (a.k.a. positions) of importance. From there, we calculate weights for these keys and multiply these weights against the values at those positions. Generating a vector with the important information for any kind of prediction we want to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current state-of-the-art (SOTA) models are much more clever when it comes to applying attention. Be on the lookout for a future post on different methods of attention and their benefits. In the meantime, if you're interested, take a look at other ways of applying attention mechanisms. Also, be sure to play around with the awesome new AI products and models continuously being released!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankie Cancino is a Senior Data Scientist for Mercedes-Benz Research & Development, working on machine learning use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "* [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "* [LinkedIn](https://www.linkedin.com/in/frankie-cancino/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
